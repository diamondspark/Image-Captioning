{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "        file = open(filename, 'r')\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "        return text\n",
    "\n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "        doc = load_doc(filename)\n",
    "        dataset = list()\n",
    "        for line in doc.split('\\n'):\n",
    "            if len(line) < 1:\n",
    "                continue\n",
    "            identifier = line.split('.')[0]\n",
    "            dataset.append(identifier)\n",
    "        return set(dataset)\n",
    "    \n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "        doc = load_doc(filename)\n",
    "        descriptions = dict()\n",
    "        for line in doc.split('\\n'):\n",
    "            tokens = line.split()\n",
    "            image_id, image_desc = tokens[0], tokens[1:]\n",
    "            if image_id in dataset:\n",
    "                if image_id not in descriptions:\n",
    "                    descriptions[image_id] = list()\n",
    "                desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "                descriptions[image_id].append(desc)\n",
    "        return descriptions\n",
    "    \n",
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "        all_features = load(open(filename, 'rb'))\n",
    "        features = {k: all_features[k] for k in dataset}\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n"
     ]
    }
   ],
   "source": [
    "# load training dataset (6K)\n",
    "filename = './../data/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('./../data/descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('./../data/features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7579\n"
     ]
    }
   ],
   "source": [
    "#TOKENIZING DESCRIPTIONS\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# convert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "        all_desc = list()\n",
    "        for key in descriptions.keys():\n",
    "            [all_desc.append(d) for d in descriptions[key]]\n",
    "        return all_desc\n",
    "\n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "        lines = to_lines(descriptions)\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(lines)\n",
    "        return tokenizer\n",
    "    \n",
    "    \n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "\n",
    "\n",
    "def create_sequences(tokenizer, max_length, descriptions, photos):\n",
    "\tX1, X2, y = list(), list(), list()\n",
    "\t# walk through each image identifier\n",
    "\tfor key, desc_list in descriptions.items():\n",
    "\t\t# walk through each description for the image\n",
    "\t\tfor desc in desc_list:\n",
    "\t\t\t# encode the sequence\n",
    "\t\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
    "\t\t\t# split one sequence into multiple X,y pairs\n",
    "\t\t\tfor i in range(1, len(seq)):\n",
    "\t\t\t\t# split into input and output pair\n",
    "\t\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
    "\t\t\t\t# pad input sequence\n",
    "\t\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "\t\t\t\t# encode output sequence\n",
    "\t\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\t\t\t\t# store\n",
    "\t\t\t\tX1.append(photos[key][0])\n",
    "\t\t\t\tX2.append(in_seq)\n",
    "\t\t\t\ty.append(out_seq)\n",
    "\treturn array(X1), array(X2), array(y)\n",
    "\n",
    "# calculate the length of the description with the most words\n",
    "def get_max_length(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\treturn max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description Length: 34\n"
     ]
    }
   ],
   "source": [
    "max_length = get_max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
